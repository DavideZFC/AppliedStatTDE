#IC.T2.X2 1.237878 1.816500
cluester.es
cluster.es
dim(df)
dim(df)
df <- read.table("diamonds.txt")
dim(df)
df <- scale(df)
dim(df)
df$clu <- cluster.es
df
dim8df
df
df <- read.table("diamonds.txt")
head(df)
df <- scale(df)
df$clu <- factor(cluster.es)
df$clu <- as.vector(cluster.es)
df <- read.table("diamonds.txt")
head(df)
df <- scale(df)
df$clu <- as.vector(cluster.es)
help(scale)
df <- read.table("diamonds.txt")
head(df)
df <- scale(df)
df <- read.table("diamonds.txt")
head(df)
df <- data.frame(scale(df))
df <- read.table("diamonds.txt")
head(df)
df <- data.frame(scale(df))
plot(df$Diameter, df$Carats, pch = 16)
df.e<- dist(df, method='euclidean') # distance matrix
## Ward linkage
df.es <- hclust(df.e, method='ward.D2') # clustering gerarchico con ward linkage
coph.es <- cophenetic(df.es)
x11()
par(mfrow= c(1,2))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='ward', asp=1 )
dev.off()
x11() # dendrogramma
plot(df.es, main='euclidean-ward', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(df.es, k=2)
dev.off()
help(cutree)
cluster.es <- cutree(df.es, k=2)
cluster.es
dim1 <- length(cluster.es[which(cluster.es == 2)]) # cluster di dimensione 38
dim2 <- length(cluster.es[which(cluster.es == 1)]) # cluster di dimensione 297
dim1
dim2
centroids <- apply (df, 2, function (x) tapply (x, cluster.es, mean))
centroids
x11()
plot(df[,1], df[,2], pch = 16, col = cluster.es)
points(centroids, col = 'blue', pch = 16)
dev.off()
df$clu <- as.vector(cluster.es)
mcshapiro.test(df[which(df$clu == 1),1:2])#pval = 0.6228
mcshapiro.test(df[which(df$clu == 2),1:2])#pval = 0.6052
var.test(X+Y~clu, data = df) #pval = 0.2206
# sono normali bivariate -> test per la differenza di medie di popolazioni normali con stessa varianza
n1 <- dim(df[which(df$clu == 1),1:2])[1]
n2 <- dim(df[which(df$clu == 2),1:2])[1]
p  <- dim(df)[2]
# we compute the sample mean, covariance matrices and the matrix
# Spooled
t1.mean <- sapply(df[which(df$clu == 1),1:2],mean)
t2.mean <- sapply(df[which(df$clu == 2),1:2],mean)
t1.cov  <-  cov(df[which(df$clu == 1),1:2])
t2.cov  <-  cov(df[which(df$clu == 2),1:2])
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)
# we compare the matrices
list(S1=t1.cov, S2=t2.cov, Spooled=Sp)
# Test H0: mu1 == mu2  vs  H1: mu1 != mu2
# i.e.,
# Test H0: mu1-mu2 == c(0,0)  vs  H1: mu1-mu2 != c(0,0)
alpha   <- .01
delta.0 <- c(0,0)
Spinv   <- solve(Sp)
T2 <- n1*n2/(n1+n2) * (t1.mean-t2.mean-delta.0) %*% Spinv %*% (t1.mean-t2.mean-delta.0)
cfr.fisher <- (p*(n1+n2-2)/(n1+n2-1-p))*qf(1-alpha,p,n1+n2-1-p)
T2 < cfr.fisher
P <- 1 - pf(T2/(p*(n1+n2-2)/(n1+n2-1-p)), p, n1+n2-1-p)
P
# P-value is null -> I reject the null hypothesis and accept the alternative one: there are two sites
# Simultaneous T2 intervals
IC.T2.X1 <- c(t1.mean[1]-t2.mean[1]-sqrt(cfr.fisher*Sp[1,1]*(1/n1+1/n2)), t1.mean[1]-t2.mean[1]+sqrt(cfr.fisher*Sp[1,1]*(1/n1+1/n2)) )
IC.T2.X2 <- c(t1.mean[2]-t2.mean[2]-sqrt(cfr.fisher*Sp[2,2]*(1/n1+1/n2)), t1.mean[2]-t2.mean[2]+sqrt(cfr.fisher*Sp[2,2]*(1/n1+1/n2)) )
IC.T2 <- rbind(IC.T2.X1, IC.T2.X2)
dimnames(IC.T2)[[2]] <- c('inf','sup')
IC.T2
#            inf      sup
#IC.T2.X1 2.676154 3.184315
#IC.T2.X2 1.237878 1.816500
mcshapiro.test(df[which(df$clu == 1),1:2])#pval = 0.6228
load("mcshapiro.test.RData")
load("mcshapiro.test.RData")
df <- read.table("diamonds.txt")
head(df)
df <- data.frame(scale(df))
plot(df$Diameter, df$Carats, pch = 16)
df.e<- dist(df, method='euclidean') # distance matrix
## Ward linkage
df.es <- hclust(df.e, method='ward.D2') # clustering gerarchico con ward linkage
coph.es <- cophenetic(df.es)
x11()
par(mfrow= c(1,2))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='ward', asp=1 )
dev.off()
x11() # dendrogramma
plot(df.es, main='euclidean-ward', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(df.es, k=2)
dev.off()
help(cutree)
cluster.es <- cutree(df.es, k=2)
cluster.es
dim1 <- length(cluster.es[which(cluster.es == 2)]) # cluster di dimensione 38
dim2 <- length(cluster.es[which(cluster.es == 1)]) # cluster di dimensione 297
dim1
dim2
centroids <- apply (df, 2, function (x) tapply (x, cluster.es, mean))
centroids
x11()
plot(df[,1], df[,2], pch = 16, col = cluster.es)
points(centroids, col = 'blue', pch = 16)
dev.off()
df$clu <- as.vector(cluster.es)
mcshapiro.test(df[which(df$clu == 1),1:2])#pval = 0.6228
mcshapiro.test(df[which(df$clu == 2),1:2])#pval = 0.6052
n1 <- dim(df[which(df$clu == 1),1:2])[1]
n2 <- dim(df[which(df$clu == 2),1:2])[1]
p  <- dim(df)[2]
# we compute the sample mean, covariance matrices and the matrix
# Spooled
t1.mean <- sapply(df[which(df$clu == 1),1:2],mean)
t2.mean <- sapply(df[which(df$clu == 2),1:2],mean)
t1.cov  <-  cov(df[which(df$clu == 1),1:2])
t2.cov  <-  cov(df[which(df$clu == 2),1:2])
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)
# we compare the matrices
list(S1=t1.cov, S2=t2.cov, Spooled=Sp)
# Test H0: mu1 == mu2  vs  H1: mu1 != mu2
# i.e.,
# Test H0: mu1-mu2 == c(0,0)  vs  H1: mu1-mu2 != c(0,0)
alpha   <- .05
delta.0 <- c(0,0)
Spinv   <- solve(Sp)
T2 <- n1*n2/(n1+n2) * (t1.mean-t2.mean-delta.0) %*% Spinv %*% (t1.mean-t2.mean-delta.0)
cfr.fisher <- (p*(n1+n2-2)/(n1+n2-1-p))*qf(1-alpha,p,n1+n2-1-p)
T2 < cfr.fisher
P <- 1 - pf(T2/(p*(n1+n2-2)/(n1+n2-1-p)), p, n1+n2-1-p)
P
# P-value is null -> I reject the null hypothesis and accept the alternative one: there are two type of diamonds
# Simultaneous T2 intervals
IC.T2.X1 <- c(t1.mean[1]-t2.mean[1]-sqrt(cfr.fisher*Sp[1,1]*(1/n1+1/n2)), t1.mean[1]-t2.mean[1]+sqrt(cfr.fisher*Sp[1,1]*(1/n1+1/n2)) )
IC.T2.X2 <- c(t1.mean[2]-t2.mean[2]-sqrt(cfr.fisher*Sp[2,2]*(1/n1+1/n2)), t1.mean[2]-t2.mean[2]+sqrt(cfr.fisher*Sp[2,2]*(1/n1+1/n2)) )
IC.T2 <- rbind(IC.T2.X1, IC.T2.X2)
dimnames(IC.T2)[[2]] <- c('inf','sup')
IC.T2
#            inf      sup
#IC.T2.X1 2.676154 3.184315
#IC.T2.X2 1.237878 1.816500
x11()
plot(df[,1], df[,2], pch = 16, col = cluster.es)
points(centroids, col = 'blue', pch = 16)
rm(list = ls())
data <- read.table("diamonds.txt")
head(data)
datastd <- data.frame(scale(data))
plot(datastd)
diam.e <- dist(datastd, method='euclidean')
diam.es <- hclust(diam.e, method='ward.D')
x11()
plot(diam.es, main='euclidean-ward', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
cluster.es <- cutree(diam.es, k=3) # euclidean-single
coph.es <- cophenetic(diam.es)
es <- cor(diam.e, coph.es)
table(cluster.es)/dim(data)[1]
alpha <- 0.05
k <- 6
plot(datastd, col = cluster.es+1)
i1 <- which(cluster.es == "1")
i2 <- which(cluster.es == "2")
i3 <- which(cluster.es == "3")
load("mcshapiro.test.RData")
mcshapiro.test(datastd[i1,])
mcshapiro.test(datastd[i2,])
mcshapiro.test(datastd[i3,])
n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
mean1 <- sapply(datastd[i1,],mean)
mean1
mean2 <- sapply(datastd[i2,],mean)
mean2
mean3 <- sapply(datastd[i3,],mean)
mean3
S1 <- cov(datastd[i1,])
S2 <- cov(datastd[i2,])
S3 <- cov(datastd[i3,])
fit <- manova(as.matrix(datastd) ~ cluster.es)
# summary.manova(fit,test="Wilks")   # Exact tests for p<=2 or g<=3 already implemented in R
W  <- (n1-1)*S1 + (n2-1)*S2 + (n3-1)*S3
W1 <- summary.manova(fit)$SS$Residuals
### question c)
alpha <- 0.05
p <- 2
g <- 3
n <- n1+n2+n3
IC1 <- cbind(mean1 - mean2 - qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n1 + 1/n2)),
mean1 - mean2,
mean1 - mean2 + qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n1 + 1/n2)))
IC2 <- cbind(mean1 - mean3 - qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n1 + 1/n3)),
mean1 - mean3,
mean1 - mean3 + qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n1 + 1/n3)))
IC3 <- cbind(mean3 - mean2 - qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n3 + 1/n2)),
mean3 - mean2,
mean3 - mean2 + qt(1-alpha/(p*g*(g-1)), n-g)*sqrt(diag(W)/(n-g) *(1/n3 + 1/n2)))
IC1
IC2
IC3
df <- read.tabel("trading.txt")
df <- read.table("trading.txt")
head(df)
df <- read.table("trading.txt")
head(df)
class <- qda(df[,1:2], df[,3])
library(MASS)
class <- qda(df[,1:2], df[,3])
summary(qda)
summary(qda)
summary(class)
class$prior
#      0        1
# 0.4661355 0.5338645
class$counts
#      0        1
# 0.4661355 0.5338645
M <- c(mean(df[which(df$gain == 0),1:2]),mean(df[which(df$gain == 1),1:2]))
#      0        1
# 0.4661355 0.5338645
M <- c(mean(df[which(df$gain == 0),1]),mean(df[which(df$gain == 1),1]))
M
M <- rbind(c(mean(df[which(df$gain == 0),1]),mean(df[which(df$gain == 1),1])),
c(mean(df[which(df$gain == 0),2]),mean(df[which(df$gain == 1),2])))
M
df
head(df)
#       gain = 0 gain = 1
# goog -3.736068 5.1162687
# aapl -0.830094 0.7081866
pred <- predict(class, df[,1:2])
table(pred$class, df[,3])
#    0  1
# 0 84 45
# 1 33 89
plot(df$google, df$apple)
x11()
par(mfrow = c(1,2))
plot(df$google, df$apple, pch = 16, col = df$gain)
plot(df$google, df$apple, pch = 16, col = pred$class)
plot(df$google, df$apple, pch = 16, col = factor(df$gain))
plot(df$google, df$apple, pch = 16, col = pred$class)
x11()
par(mfrow = c(1,2))
plot(df$google, df$apple, pch = 16, col = factor(df$gain), main = 'true')
plot(df$google, df$apple, pch = 16, col = pred$clas, main = 'predicted')
x11()
par(mfrow = c(1,2))
plot(df$google, df$apple, pch = 16, col = factor(df$gain), main = 'true')
plot(df$google, df$apple, pch = 16, col = pred$clas, main = 'predicted')
shapiro.test(df$google)
shapiro.test(df$apple)
n <- dim(df)[1]
dummy <- rep(1,n)
table(dummy, df[,3])
dummy <- rep(1,n)
errors <- (dummy != df[,3])
sum(errors)/n
APER.qda <- sum(errors)/n
APER.qda
errors <- (pred$class != df[,3])
APER.qda <- sum(errors)/n
APER.qda
class <- qda(df[,1:2], df[,3], CV = TRUE)
pred <- predict(class, df[,1:2])
table(pred$class, df[,3])
#    0  1
# 0 84 45
# 1 33 89
errors <- (pred$class != df[,3])
APER.qda <- sum(errors)/n
class <- qda(df[,1:2], df[,3], CV = TRUE)
pred <- predict(class, df[,1:2])
table(pred$class, df[,3])
errors <- (pred$class != df[,3])
APER.qdaCV <- sum(errors)/n
class <- qda(df[,1:2], df[,3], CV = TRUE)
classCV <- qda(df[,1:2], df[,3], CV = TRUE)
table(classCV$class, df[,3])
errors <- (pred$class != df[,3])
APER.qdaCV <- sum(errors)/n
# 0.310757
errors <- (classCV$class != df[,3])
APER.qdaCV <- sum(errors)/n
APER.qdaCV
rm(list = ls())
data <- read.table("trading.txt")
head(data)
library(MASS)
iris2 <- data[,1:2]
species.name <- as.factor(data$gain)
qda.iris <- qda(iris2, species.name)
plot(iris2, col = species.name)
i1 <- which(species.name==1)
i2 <- which(species.name==0)
n1 <- length(i1)
n2 <- length(i2)
n <- n1+n2
# b)
Qda.iris <- predict(qda.iris, iris2)
table(classe.vera=species.name, classe.allocata=Qda.iris$class)
erroriq <- (Qda.iris$class != species.name)
APERq   <- sum(erroriq)/length(species.name)
APERq
QdaCV.iris <- qda(iris2, species.name, CV=T)
table(classe.vera=species.name, classe.allocataCV=QdaCV.iris$class)
erroriqCV <- (QdaCV.iris$class != species.name)
AERqCV   <- sum(erroriqCV)/length(species.name)
AERqCV
n2/n # trivial classifier error?
x11()
plot(iris2, pch=20)
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(qda.iris$means, col=c('red','green'), pch=4, lwd=2, cex=1.5)
x  <- seq(min(iris2[,1]), max(iris2[,1]), length=200)
y  <- seq(min(iris2[,2]), max(iris2[,2]), length=200)
xy <- expand.grid(google=x, apple=y)
z  <- predict(qda.iris, xy)$post
z1 <- z[,1] - z[,2]
z2 <- z[,2] - z[,1]
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
# c)
newdatum <- data.frame(google = -3, apple = -1.2)
predict(qda.iris, newdatum)
predict(class, newdatum)
newdatum <- data.frame(google = -3, apple = -1.2)
predict(class, newdatum)
library(MASS)
df <- read.table("trading.txt")
n <- dim(df)[1]
head(df)
class <- qda(df[,1:2], df[,3])
class$prior
#      0        1
# 0.4661355 0.5338645
M <- rbind(c(mean(df[which(df$gain == 0),1]),mean(df[which(df$gain == 1),1])),
c(mean(df[which(df$gain == 0),2]),mean(df[which(df$gain == 1),2])))
#       gain = 0 gain = 1
# goog -3.736068 5.1162687
# aapl -0.830094 0.7081866
pred <- predict(class, df[,1:2])
table(pred$class, df[,3])
#    0  1
# 0 84 45
# 1 33 89
errors <- (pred$class != df[,3])
APER.qda <- sum(errors)/n
# 0.310757
x11()
par(mfrow = c(1,2))
plot(df$google, df$apple, pch = 16, col = factor(df$gain), main = 'true')
plot(df$google, df$apple, pch = 16, col = pred$clas, main = 'predicted')
dummy <- rep(1,n)
errors <- (dummy != df[,3])
APER.dummy <- sum(errors)/n
# 0.4661355
classCV <- qda(df[,1:2], df[,3], CV = TRUE)
table(classCV$class, df[,3])
#    0  1
# 0 82 46
# 1 35 88
errors <- (classCV$class != df[,3])
APER.qdaCV <- sum(errors)/n
# 0.3227092
newdatum <- data.frame(google = -3, apple = -1.2)
predict(class, newdatum)
df <- read.table("areaC.txt")
head(df)
colnames(df)
attach(df)
mod <- lm(Total ~ Weekend + Petrol + Diesel + Electric + GPL + Natural.Gas + Hybrid.Diesel + Hybrid.Petrol)
summary(mod)
plot(mod)
par(mfrow = c(2,2))
plot(mod)
shapiro.test(mod$residuals)
coefs <- coef(mod)
coefs
sigma   <- sd(mod$residuals)
# -0.1877149
beta.00 <- coefs[1]
beta.01 <- coefs[1] + coefs[2]
beta.1  <- coefs[3]
beta.2  <- coefs[4]
beta.3  <- coefs[5]
beta.4  <- coefs[6]
beta.5  <- coefs[7]
beta.6  <- coefs[8]
beta.7  <- coefs[9]
sigma   <- sd(mod$residuals)
summary(mod)
# first of all, only Petrol and Natural.Gas provide a significance with a confidence of over 95%, many variables
# are with high chance not influent
vif(mod)
par(mfrow = c(2,2))
plot(mod)
shapiro.test(mod$residuals)
# residuals are not considerable normal with a confidence of 95%, moreover we can see a solid leverage effect
attach(df)
result.pc <- princomp(cbind(Weekend,Petrol,Diesel,Electric,GPL, Natural.Gas, Hybrid.Diesel, Hybrid.Petrol), scores=TRUE)
summary(result.pc)
result.pc$load
# Loadings
par(mar = c(1,4,0,2), mfrow = c(4,1))
for(i in 1:4)barplot(result.pc$load[,i], ylim = c(-1, 1))
result.pc <- princomp(cbind(Weekend,Petrol,Diesel,Electric,GPL, Natural.Gas, Hybrid.Diesel, Hybrid.Petrol), scores=TRUE)
summary(result.pc)
result.pc$load
# I choose the first two principal components
pc1 <- result.pc$scores[,1]
pc2 <- result.pc$scores[,2]
mod2 <- lm(Total ~ pc1 + pc2)
summary(mod2)
# residuals are not considerable normal with a confidence of 95%, moreover we can see a solid leverage effect
detach(df)
pca <- princomp(df[,1:7], scores=TRUE)
summary(pca)
biplot(pca)
pca$sdev^2/sum(pca$sdev^2)
x11()
biplot(pca)
pca$sdev^2/sum(pca$sdev^2)
loading <- pca$loadings
x11()
par(mar = c(1,4,0,2), mfrow = c(6,1))
for(i in 1:6) barplot(loading[,i], ylim = c(-1, 1))
m <- sapply(data[,1:7],mean)
m <- sapply(df[,1:7],mean)
m <- sapply(df[,1:7],mean)
sp1.pc <- pca$scores[,1]
sp2.pc <- pca$scores[,2]
fm.pc1 <- lm(data$Total ~ data$Weekend + sp1.pc + sp2.pc)
summary(fm.pc1)
x11()
par(mfrow = c(2,2))
plot(fm.pc1)
shapiro.test(fm.pc1$residuals)
betas <- coefficients(fm.pc1)[3:4]%*%t(pca$loadings[,1:2])
beta0 <- coefficients(fm.pc1)[1] - betas%*%m
beta0
coefficients(fm.pc1)[2]
betas
m <- sapply(df[,1:7],mean)
sp1.pc <- pca$scores[,1]
sp2.pc <- pca$scores[,2]
fm.pc1 <- lm(df$Total ~ df$Weekend + sp1.pc + sp2.pc)
summary(fm.pc1)
x11()
par(mfrow = c(2,2))
plot(fm.pc1)
shapiro.test(fm.pc1$residuals)
betas <- coefficients(fm.pc1)[3:4]%*%t(pca$loadings[,1:2])
beta0 <- coefficients(fm.pc1)[1] - betas%*%m
beta0
coefficients(fm.pc1)[2]
betas
summary(fm.pc1)
library(car)
A <- rbind(c(0,1,0,0), c(0,0,0,1))
b <- c(0,0)
linearHypothesis(fm.pc1, A, b)
fm.pc2 <- lm(data$Total ~ sp1.pc)
summary(fm.pc2)
x11()
par(mfrow=c(2,2))
plot(fm.pc2)
shapiro.test(fm.pc2$residuals)
betas2 <- coefficients(fm.pc2)[2]%*%t(pca$loadings[,1])
beta02 <- coefficients(fm.pc2)[1] - betas2%*%m
beta02
betas2
library(car)
A <- rbind(c(0,1,0,0), c(0,0,0,1))
b <- c(0,0)
linearHypothesis(fm.pc1, A, b)
fm.pc2 <- lm(df$Total ~ sp1.pc)
summary(fm.pc2)
x11()
par(mfrow=c(2,2))
plot(fm.pc2)
shapiro.test(fm.pc2$residuals)
betas2 <- coefficients(fm.pc2)[2]%*%t(pca$loadings[,1])
beta02 <- coefficients(fm.pc2)[1] - betas2%*%m
beta02
betas2
