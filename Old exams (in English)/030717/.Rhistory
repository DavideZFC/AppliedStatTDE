barplot(rep(M,4), names.arg=levels(distr_benz), ylim=c(0,24), main='No factor')
barplot(rep(Mdistr,each=2), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('blue','red'),each=2), main='Only Fact. Stat.')
barplot(rep(Mbenz,times=2), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('darkgreen','orange'),times=2), main='Only Fact. Gas')
barplot(c(Mdistr[1]+Mbenz[1]-M, Mdistr[1]+Mbenz[2]-M, Mdistr[2]+Mbenz[1]-M,
Mdistr[2]+Mbenz[2]-M), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('darkgreen','orange'),times=2), density=rep(10,4), angle=135,
main='Additive model Stat.+Gas')
barplot(c(Mdistr[1]+Mbenz[1]-M, Mdistr[1]+Mbenz[2]-M, Mdistr[2]+Mbenz[1]-M,
Mdistr[2]+Mbenz[2]-M), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('blue','red'),each=2), density=rep(10,4), add=T)
barplot(Mdistr_benz, names.arg=levels(distr_benz), ylim=c(0,24),
col=rainbow(5)[2:5], main='Model with Interact. Stat.+Gas.')
plot(distr_benz, km, col=rainbow(5)[2:5], ylim=c(0,24),xlab='')
anova(lm(value ~ city, data = df))
anova(lm(value ~ type + type:city, data = df))
anova(lm(value ~ type + city + type:city, data = df))
anova(lm(value ~ type + city, data = df))
kvalue          <- df$value
kcity      <- factor(df$city)
kbenz        <- factor(df$type)
g <- length(levels(kcity))
b <- length(levels(kbenz))
n <- length(kvalue)/(g*b)
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      city, mean)
Mtype      <- tapply(kvalue,       type, mean)
SStype <- sum(n*b*(Mtype - M)^2)              # or from the summary: 1.53
SScity  <- sum(n*g*(Mcity  - M)^2)              # or from the summary: 66.70
SSres   <- sum((value - M)^2) - (SStype+SScity)   # or from the summary: 16.37
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
kvalue          <- df$value
kcity      <- factor(df$city)
kbenz        <- factor(df$type)
g <- length(levels(kcity))
b <- length(levels(kbenz))
n <- length(kvalue)/(g*b)
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      kcity, mean)
Mtype      <- tapply(kvalue,       ktype, mean)
SStype <- sum(n*b*(Mtype - M)^2)              # or from the summary: 1.53
SScity  <- sum(n*g*(Mcity  - M)^2)              # or from the summary: 66.70
SSres   <- sum((value - M)^2) - (SStype+SScity)   # or from the summary: 16.37
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
kvalue          <- df$value
kcity      <- factor(df$city)
ktype        <- factor(df$type)
g <- length(levels(kcity))
b <- length(levels(kbenz))
n <- length(kvalue)/(g*b)
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      kcity, mean)
Mtype      <- tapply(kvalue,       ktype, mean)
SStype <- sum(n*b*(Mtype - M)^2)              # or from the summary: 1.53
SScity  <- sum(n*g*(Mcity  - M)^2)              # or from the summary: 66.70
SSres   <- sum((value - M)^2) - (SStype+SScity)   # or from the summary: 16.37
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      kcity, mean)
Mtype      <- tapply(kvalue,       ktype, mean)
SStype <- sum(n*b*(Mtype - M)^2)              # or from the summary: 1.53
SScity  <- sum(n*g*(Mcity  - M)^2)              # or from the summary: 66.70
SSres   <- sum((kvalue - M)^2) - (SStype+SScity)   # or from the summary: 16.37
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
SSdistr <- sum(n*b*(Mdistr - M)^2)              # or from the summary: 1.53
SSbenz  <- sum(n*g*(Mbenz  - M)^2)              # or from the summary: 66.70
SSres   <- sum((km - M)^2) - (SSdistr+SSbenz)   # or from the summary: 16.37
Ftot      <- ( (SSdistr + SSbenz) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
df <- read.table("kimono.txt")
head(df)
tokyo <- df[which(df$city == 'Tokyo'),1]
kyoto <- df[which(df$city == 'Kyoto'),1]
hm <- df[which(df$type == 'hand-made'),1]
rtu <- df[which(df$type == 'ready-to-use'),1]
shapiro.test(tokyo)
shapiro.test(kyoto) # sicuramente non valgono le ipotesi della anova dividendo per città
shapiro.test(hm) # ok
shapiro.test(rtu) # confidenza inferiore al 95%, proviamo lo stesso a formulare il modello
qqnorm(rtu)
qqline(rtu)
bartlett.test(df$value , df$type)
var.test(hm,rtu) # l'ipotesi sulle varianze è accettabile
anova(lm(value ~ type, data = df))
# Analysis of Variance Table
# Response: value
# Df  Sum Sq Mean Sq F value    Pr(>F)
# type        1 29342.6   29343   14554 < 2.2e-16 ***
# Residuals 526  1060.5       2
# ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# ok l'anova ci indica fortemente che il prezzo è influenzato dalla tipologia
# vediamo un modello completo con eventuali interazioni
anova(lm(value ~ type + city + type:city, data = df))
# Analysis of Variance Table
# Response: value
# Df  Sum Sq Mean Sq    F value Pr(>F)
# type        1 29342.6 29342.6 14528.1695 <2e-16 ***
#   city        1     1.9     1.9     0.9305 0.3352
# type:city   1     0.3     0.3     0.1240 0.7249
# Residuals 524  1058.3     2.0
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# l'interazione non sembra influire, la togliamo
anova(lm(value ~ type + city, data = df))
# Analysis of Variance Table
#
# Response: value
# Df  Sum Sq Mean Sq   F value Pr(>F)
# type        1 29342.6 29342.6 14552.451 <2e-16 ***
#   city        1     1.9     1.9     0.932 0.3348
# Residuals 525  1058.6     2.0
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# solo la variabile type pare influire sul value, il nostro modello ridotto comprenderà solo quella
### Note: These aren't the only tests we can do!
### Example: global test for the significance of the two treatments
###          (model without interaction)
kvalue          <- df$value
kcity      <- factor(df$city)
ktype        <- factor(df$type)
g <- length(levels(kcity))
b <- length(levels(kbenz))
n <- length(kvalue)/(g*b)
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      kcity, mean)
Mtype      <- tapply(kvalue,       ktype, mean)
SStype <- sum(n*b*(Mtype - M)^2)
SScity  <- sum(n*g*(Mcity  - M)^2)
SSres   <- sum((kvalue - M)^2) - (SStype+SScity)
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
n
n <- length(kvalue)/(g*b)
b <- length(levels(kbenz))
kvalue          <- df$value
kcity      <- factor(df$city)
ktype        <- factor(df$type)
g <- length(levels(kcity))
b <- length(levels(ktype))
n <- length(kvalue)/(g*b)
M           <- mean(kvalue)
Mcity      <- tapply(kvalue,      kcity, mean)
Mtype      <- tapply(kvalue,       ktype, mean)
SStype <- sum(n*b*(Mtype - M)^2)
SScity  <- sum(n*g*(Mcity  - M)^2)
SSres   <- sum((kvalue - M)^2) - (SStype+SScity)
Ftot      <- ( (SStype + SScity) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
km          <- c(18.7, 16.8, 20.1, 22.4, 14.0, 15.2, 22.0, 23.3)
distr       <- factor(c('Esso','Esso','Esso','Esso','Shell','Shell','Shell','Shell'))
benz        <- factor(c('95','95','98','98','95','95','98','98'))
distr_benz  <- factor(c('Esso95','Esso95','Esso98','Esso98','Shell95','Shell95','Shell98','Shell98'))
g <- length(levels(distr))
b <- length(levels(benz))
n <- length(km)/(g*b)
M           <- mean(km)
Mdistr      <- tapply(km,      distr, mean)
Mbenz       <- tapply(km,       benz, mean)
Mdistr_benz <- tapply(km, distr_benz, mean)
x11()
par(mfrow=c(2,3),las=2)
barplot(rep(M,4), names.arg=levels(distr_benz), ylim=c(0,24), main='No factor')
barplot(rep(Mdistr,each=2), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('blue','red'),each=2), main='Only Fact. Stat.')
barplot(rep(Mbenz,times=2), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('darkgreen','orange'),times=2), main='Only Fact. Gas')
barplot(c(Mdistr[1]+Mbenz[1]-M, Mdistr[1]+Mbenz[2]-M, Mdistr[2]+Mbenz[1]-M,
Mdistr[2]+Mbenz[2]-M), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('darkgreen','orange'),times=2), density=rep(10,4), angle=135,
main='Additive model Stat.+Gas')
barplot(c(Mdistr[1]+Mbenz[1]-M, Mdistr[1]+Mbenz[2]-M, Mdistr[2]+Mbenz[1]-M,
Mdistr[2]+Mbenz[2]-M), names.arg=levels(distr_benz), ylim=c(0,24),
col=rep(c('blue','red'),each=2), density=rep(10,4), add=T)
barplot(Mdistr_benz, names.arg=levels(distr_benz), ylim=c(0,24),
col=rainbow(5)[2:5], main='Model with Interact. Stat.+Gas.')
plot(distr_benz, km, col=rainbow(5)[2:5], ylim=c(0,24),xlab='')
### Two-ways ANOVA
###----------------
### Model with interaction (complete model):
### X.ijk = mu + tau.i + beta.j + gamma.ij + eps.ijk; eps.ijk~N(0,sigma^2),
###     i=1,2 (effect station), j=1,2 (effect gasoline)
fit.aov2.int <- aov(km ~ distr + benz + distr:benz)
summary.aov(fit.aov2.int)
### Test:
### 1) H0: gamma.11 = gamma.12 = gamma.21 = gamma.22 = 0    vs   H1: (H0)^c
###    i.e.,
###    H0: There is no significant interaction between the factors station
###        and gasoline in terms of performances
###    H1: There exists a significant interaction between the factors station
###        and gasoline in terms of performances
###
### 2) H0: tau.1 = tau.2 = 0    vs   H1: (H0)^c
###    i.e.,
###    H0: The effect "gas station" doesn't significantly influence performances
###    H1: The effect "gas station" significantly influences performances
###
### 3) H0: beta.1 = beta.2 = 0    vs   H1: (H0)^c
###    i.e.,
###    H0: The effect "gasoline" doesn't significantly influence performances
###    H1: The effect "gasoline" significantly influences performances
# Test 1): Let's focus on the row of the summary distr:benz :
#             Df Sum Sq Mean Sq F value  Pr(>F)
# distr:benz   1  10.35   10.35   6.884 0.05857 .
# The P-value of test 1) is 0.05857. Reject at 10%, dont't reject at 1%,5% -> ?
# Test 2): Let's focus on the row of the summary distr:
#             Df Sum Sq Mean Sq F value  Pr(>F)
# distr        1   1.53    1.53   1.018 0.37001
# The P-value of test 2) is 0.37001. Don't reject at 10%, 5%, 1% -> not significant
# Test 3): Let's focus on the row of the summary benz:
#             Df Sum Sq Mean Sq F value  Pr(>F)
# benz         1  66.70   66.70  44.357 0.00264 **
# The P-value of test 3) is 0.00264. Reject at 10%, 5%, 1% -> significant
# Point b)
# From test 1): We don't have strong evidence that the interaction has effect
# => try to remove the interaction term and estimate the model without interaction
### Additive model:
### X.ijk = mu + tau.i + beta.j + eps.ijk; eps.ijk~N(0,sigma^2),
###     i=1,2 (effect station), j=1,2 (effect gasoline)
fit.aov2.ad <- aov(km ~ distr + benz)
summary.aov(fit.aov2.ad)
# Remark: by removing the interaction, the residual degrees of freedom increase!
# Test: 2bis) H0: tau.1 = tau.2 = 0    vs   H1: (H0)^c
# From the summary:
#             Df Sum Sq Mean Sq F value  Pr(>F)
# distr        1   1.53    1.53   0.468 0.52440
# The P-value of test 2bis) is 0.52440. Don't reject at 10%, 5%, 1% -> not significant
# Test: 3bis) H0: beta.1 = beta.2 = 0    vs   H1: (H0)^c
# From the summary:
#             Df Sum Sq Mean Sq F value  Pr(>F)
# benz         1  66.70   66.70  20.378 0.00632 **
# The P-value of test 2bis) is 0.00632. Dont' reject at 10%, 5%, 1% -> significant
### Note: These aren't the only tests we can do!
### Example: global test for the significance of the two treatments
###          (model without interaction)
SSdistr <- sum(n*b*(Mdistr - M)^2)              # or from the summary: 1.53
SSbenz  <- sum(n*g*(Mbenz  - M)^2)              # or from the summary: 66.70
SSres   <- sum((km - M)^2) - (SSdistr+SSbenz)   # or from the summary: 16.37
Ftot      <- ( (SSdistr + SSbenz) / ((g-1)+(b-1)))/(SSres / (n*g*b-g-b+1))
Ptot      <- 1 - pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
Ptot      <- pf(Ftot, (g-1)+(b-1), n*g*b-g-b+1) # attention to the dgf!
Ptot
fit.aov1 <- aov(kvalue ~ ktype)
summary.aov(fit.aov1)
SSres <- sum(residuals(fit.aov1)^2)
### Interval at 95% for the differences (reduced additive model)
### [b=2, thus one interval only]
IC <- c(diff(Mtype) - qt(0.975, (n*g-1)*b) * sqrt(SSres/((n*g-1)*b) *(1/(n*g) + 1/(n*g))),
diff(Mtype) + qt(0.975, (n*g-1)*b) * sqrt(SSres/((n*g-1)*b) *(1/(n*g) + 1/(n*g))))
names(IC) <- c('Inf', 'Sup')
IC    # IC for mu(hand-made)-mu(ready-to-use)
### Interval at 95% for the differences (reduced additive model)
### [b=2, thus one interval only]
IC <- c(diff(Mtype) - qt(0.975, (n*g-1)*b) * sqrt(SSres/((n*g-1)*b) *(1/(n*g) + 1/(n*g))),
diff(Mtype) + qt(0.975, (n*g-1)*b) * sqrt(SSres/((n*g-1)*b) *(1/(n*g) + 1/(n*g))))
names(IC) <- c('Inf', 'Sup')
IC
df <- read.table('geisha.txt')
head(df)
plot(duration, time, data = df)
plot(df$duration, df$time)
plot(df$duration, df$time, pch = 16)
abline(y = x)
help(abline)
abline(a = 0, b = 1)
x11()
image(1:150,1:150,as.matrix(df), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
x11()
image(1:n,1:n,as.matrix(df), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
n = dim(df)[1]
x11()
image(1:n,1:n,as.matrix(df), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
df.e<- dist(df, method='euclidean')
x11()
image(1:n,1:n,as.matrix(df.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
x11()
image(1:n,1:n,as.matrix(df.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
misc <- sample(n)
df <- df[misc,]
df.e<- dist(df, method='euclidean')
x11()
image(1:n,1:n,as.matrix(df.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
x11()
plot(df$duration, df$time, pch = 16)
df.es <- hclust(df.e, method='single') # clustering gerarchico con single linkage
df.es$merge
help(hclust) # vediamo gli attributi
df.es$merge
df.es$height
df.es$order
df.es$labels
x11() # dendrogramma
plot(df.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(df.es, k=2)
cluster.es <- cutree(df.es, k=2)
cluster.es
n
coph.es <- cophenetic(df.es)
coph.es
x11()
layout(rbind(c(0,1,0),c(2,3,4)))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(df.es), main='Single', asp=1 )
x11()
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(df.es), main='Single', asp=1 )
image(as.matrix(df.es), main='Single', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
x11()
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
help(layout)
x11()
par(mfrow= c(1,2))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
View(df)
cluster.es # cluster osceno, solo il 53 viene classificato in un cluster diverso dagli altri
cluster.es[which(cluster.es == 2)]
cluster.es[which(cluster.es == 1)]
sapply(unique(cluster.es), clust.centroid, df.e, cluster.es)
clust.centroid = function(i, dat, clusters) {
ind = (clusters == i)
colMeans(dat[ind,])
}
sapply(unique(cluster.es), clust.centroid, df.e, cluster.es)
cluster.es
unique(cluster.es)
cluster == 1
cluster.es == 1
ind = (clusters == j)
for (j in i){
ind = (clusters == j)
colMeans(dat[ind,])
}
clust.centroid = function(i, dat, clusters) {
for (j in i){
ind = (clusters == j)
colMeans(dat[ind,])
}
}
sapply(unique(cluster.es), clust.centroid, df.e, cluster.es)
print(j)
clust.centroid = function(i, dat, clusters) {
for (j in i){
print(j)
ind = (clusters == j)
colMeans(dat[ind,])
}
}
sapply(unique(cluster.es), clust.centroid, df.e, cluster.es)
clust.centroid = function(i, dat, clusters) {
for (j in i){
print(j)
ind = (clusters == j)
print(ind)
colMeans(dat[ind,])
}
}
sapply(unique(cluster.es), clust.centroid, df.e, cluster.es)
apply (df.e, 2, function (x) tapply (x, cluster.es, mean))
apply (df.e, 2, function (x) tapply (x, cluster.es, mean))
apply (df.e, 1, function (x) tapply (x, cluster.es, mean))
df.e[cluster.es == 1,]
# cluster osceno, solo il 53 viene classificato in un cluster diverso dagli altri
# per sport vediamo i centroidi dei cluster
cluster.es == 1
df.e
df <- read.table('geisha.txt')
head(df)
# mischiamo i dati
misc <- sample(n)
df <- df[misc,]
n = dim(df)[1]
df.e<- dist(df, method='euclidean') # distance matrix
df.es <- hclust(df.e, method='single') # clustering gerarchico con single linkage
coph.es <- cophenetic(df.es)
coph.es
x11()
par(mfrow= c(1,2))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
help(hclust) # vediamo gli attributi
df.es$merge
df.es$height
df.es$order
df.es$labels
x11() # dendrogramma
plot(df.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(df.es, k=2) # ok fa schifo sto clustering
help(cutree)
cluster.es <- cutree(df.es, k=2)
cluster.es
cluster.es[which(cluster.es == 2)] # cluster di dimensione 1
cluster.es[which(cluster.es == 1)]
cluster.es[which(cluster.es == 2)] # cluster di dimensione 1
cluster.es[which(cluster.es == 1)]
cluster.es[which(cluster.es == 1)] # cluster di dimensione 159
cluster.es[which(cluster.es == 2)] # cluster di dimensione 1
df.e[cluster.es == 1,]
df.e
apply (df, 2, function (x) tapply (x, cluster.es, mean))
dim1 <- length(cluster.es[which(cluster.es == 2)]) # cluster di dimensione 1
dim2 <- length(cluster.es[which(cluster.es == 1)]) # cluster di dimensione 159
dim1
dim2
centroids <- apply (df, 2, function (x) tapply (x, cluster.es, mean))
centroids
points(centroids, col = 'red')
x11()
plot(df$duration, df$time, pch = 16)
points(centroids, col = 'red')
points(centroids, col = 'red', pch = 11)
x11()
plot(df$duration, df$time, pch = 16)
points(centroids, col = 'red', pch = 11)
df.es <- hclust(df.e, method='complete') # clustering gerarchico con single linkage
help(hclust) # vediamo gli attributi
df.es$merge
df.es$height
df.es$order
df.es$labels
coph.es <- cophenetic(df.es)
coph.es
x11()
par(mfrow= c(1,2))
image(as.matrix(df.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
x11() # dendrogramma
plot(df.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(df.es, k=2) # ok fa schifo sto clustering
help(cutree)
cluster.es <- cutree(df.es, k=2)
cluster.es
dim1 <- length(cluster.es[which(cluster.es == 2)]) # cluster di dimensione 1
dim2 <- length(cluster.es[which(cluster.es == 1)]) # cluster di dimensione 159
dim1
dim2
# cluster osceno, solo il 53 viene classificato in un cluster diverso dagli altri
# per sport vediamo i centroidi dei cluster
centroids <- apply (df, 2, function (x) tapply (x, cluster.es, mean))
centroids
x11()
plot(df$duration, df$time, pch = 16)
points(centroids, col = 'red', pch = 11)
win[cluster.es[which(cluster.es == 1)]] <- 1
win <- rep(NA, n)
win[cluster.es[which(cluster.es == 1)]] <- 1
win
cluster.es
win[which(cluster.es == 1)] <- 1
win
df$winner <- win
x11()
plot(df$duration, df$time, pch = 16)
points(centroids, col = 'red', pch = 11)
qqnorm(df[which(df$winner == 1),1])
shapiro.test(df[which(df$winner == 1),1])
head(df)
dur       <- df$dur
time      <- df$time
win   <- factor(df$winner)
g <- length(levels(win))
gn <- length(kvalue)/(g*b)
g
n <- length(dur)/g
win   <- factor(df$winner)
df$winner <- win
df$winner
win[which(cluster.es == 2)] <- 0 # hanno perso
win[which(cluster.es == 1)] <- 1 # hanno vinto
win[which(cluster.es == 2)] <- 0 # hanno perso
# consideriamo il cluster 1 (dim. 75) come quelli che hanno vinto al gioco
cluster.es
win <- rep(NA, n)
win[which(cluster.es == 1)] <- 1 # hanno vinto
win[which(cluster.es == 2)] <- 0 #
df$winner <- win
dur       <- df$dur
time      <- df$time
win   <- factor(df$winner)
g <- length(levels(win))
n <- length(dur)/g
g
D <- data.frame(diff_dur = df[which(df$winner == 1),]$duration - df[which(df$winner == 0),]$duration,
diff_time = df[which(df$winner == 1),]$time - df[which(df$winner == 0),]$time,
winner_dur = df[which(df$winner == 1),]$duration,
winner_time = df[which(df$winner == 1),]$time)
dur
dur[which(winner == 1)]
Mdur           <- mean(dur)
Mtime          <- mean(time)
Mwin_dur       <- tapply(dur, win, mean)
Mwin_time      <- tapply(time,win, mean)
Mdur
Mwin_dur
a*(a-1)/2
a <-4
a*(a-1)/2
